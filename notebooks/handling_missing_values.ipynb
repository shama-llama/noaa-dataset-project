{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Dataset: Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process for cleaning and preparing the NOAA dataset, with a focus on addressing missing values. The raw dataset contains data from three oceanic buoys, each with different sensor capabilities, leading to two distinct types of missing data: systematic and incidental.\n",
    "\n",
    "- **Systematic missingness** occurs when a station lacks the hardware to measure a certain variable. In our case, there is a buoy that never reports wind speed which implies it doesn't have an anemometer. \n",
    "\n",
    "- **Incidental missingness** refers to small, random gaps in data from a sensor that is otherwise operational, often due to transient sensor malfunctions or transmission errors.\n",
    "\n",
    "The strategy used here is to first handle the systematic gaps by partitioning the data into analysis-specific subsets. Then, incidental gaps within these subsets are addressed using time-series interpolation. This preserves data integrity by not attempting to impute values that were never physically measurable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"../data/source/NOAA_46041.csv\",\n",
    "    \"../data/source/NOAA_46050.csv\",\n",
    "    \"../data/source/NOAA_46243.csv\",\n",
    "]\n",
    "\n",
    "dataframes = [pd.read_csv(file) for file in files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standardize and Clean Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names are programmatically standardized for consistency and ease of access. This involves removing leading/trailing whitespace, replacing units and special characters with descriptive suffixes, replacing spaces with underscores, and converting all names to lowercase. A standardized naming convention is best practice to prevent errors, improve code readability, and make data manipulation more predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>station_id\n",
       "latitude_degrees_north\n",
       "longitude_degrees_east\n",
       "date_time\n",
       "air_temperature_celsius\n",
       "sea_level_pressure_hpa\n",
       "wind_speed_mps\n",
       "gust_speed_mps\n",
       "significant_wave_height_m\n",
       "dominant_wave_period_s\n",
       "sea_surface_temperature_celsius\n",
       "wind_speed_cwind_mps</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean up column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df.columns = df.columns.str.replace(\n",
    "    r\"\\s*\\(degrees north\\)\", \"_degrees_north\", regex=True\n",
    ")\n",
    "df.columns = df.columns.str.replace(r\"\\s*\\(degrees east\\)\", \"_degrees_east\", regex=True)\n",
    "df.columns = df.columns.str.replace(r\"\\s*\\(CÂ°\\)\", \"_celsius\", regex=True)\n",
    "df.columns = df.columns.str.replace(r\"\\s*\\(m/s\\)\", \"_mps\", regex=True)\n",
    "df.columns = df.columns.str.replace(r\"\\s*\\(hPa\\)\", \"_hpa\", regex=True)\n",
    "df.columns = df.columns.str.replace(r\"\\s*\\(s\\)\", \"_s\", regex=True)\n",
    "df.columns = df.columns.str.replace(r\"\\s*\\(m\\)\", \"_m\", regex=True)\n",
    "\n",
    "df.columns = df.columns.str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "df.columns = df.columns.str.replace(r\"_{2,}\", \"_\", regex=True)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "columns_str = \"\\n\".join(df.columns)\n",
    "\n",
    "html = f\"<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>{columns_str}</pre>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge Redundant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During exploratory data analysis, it was discovered that two columns, `wind_speed_mps` and `wind_speed_cwind_mps`, measure the same physical quantity but are populated for different sets of records. To create a single, authoritative source for wind speed, these columns are merged. The `combine_first` method is used to coalesce the two columns, filling NaN values in the first column with the corresponding values from the second. The original, now-redundant columns are then dropped to simplify the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'><class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 7219 entries, 0 to 7218\n",
       "Data columns (total 11 columns):\n",
       " #   Column                           Non-Null Count  Dtype              \n",
       "---  ------                           --------------  -----              \n",
       " 0   station_id                       7219 non-null   int64              \n",
       " 1   latitude_degrees_north           7219 non-null   float64            \n",
       " 2   longitude_degrees_east           7219 non-null   float64            \n",
       " 3   date_time                        7219 non-null   datetime64[ns, UTC]\n",
       " 4   air_temperature_celsius          723 non-null    float64            \n",
       " 5   sea_level_pressure_hpa           723 non-null    float64            \n",
       " 6   gust_speed_mps                   723 non-null    float64            \n",
       " 7   significant_wave_height_m        2167 non-null   float64            \n",
       " 8   dominant_wave_period_s           2167 non-null   float64            \n",
       " 9   sea_surface_temperature_celsius  2168 non-null   float64            \n",
       " 10  wind_speed                       5043 non-null   float64            \n",
       "dtypes: datetime64[ns, UTC](1), float64(9), int64(1)\n",
       "memory usage: 620.5 KB\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"wind_speed\"] = df[\"wind_speed_mps\"].combine_first(df[\"wind_speed_cwind_mps\"])\n",
    "\n",
    "df.drop(columns=[\"wind_speed_mps\", \"wind_speed_cwind_mps\"], inplace=True)\n",
    "\n",
    "buffer = io.StringIO()\n",
    "df.info(buf=buffer)\n",
    "info = buffer.getvalue()\n",
    "\n",
    "html = f\"<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>{info}</pre>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantify Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantitative analysis of missing data is performed to guide the imputation strategy. The percentage of missing values for each variable is calculated. The results clearly show that variables like `air_temperature_celsius` and `sea_level_pressure_hpa` are missing in over 65% of records. It's a direct result of specific stations lacking the necessary sensors. This confirms the presence of systematic missingness and invalidates the use of simple, global imputation methods, which would introduce significant bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>                                 Missing Values  Percentage\n",
       "station_id                                    0    0.000000\n",
       "latitude_degrees_north                        0    0.000000\n",
       "longitude_degrees_east                        0    0.000000\n",
       "date_time                                     0    0.000000\n",
       "air_temperature_celsius                    6496   89.984762\n",
       "sea_level_pressure_hpa                     6496   89.984762\n",
       "gust_speed_mps                             6496   89.984762\n",
       "significant_wave_height_m                  5052   69.981992\n",
       "dominant_wave_period_s                     5052   69.981992\n",
       "sea_surface_temperature_celsius            5051   69.968140\n",
       "wind_speed                                 2176   30.142679</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "missing_info = pd.DataFrame(\n",
    "    {\"Missing Values\": missing_data, \"Percentage\": missing_percentage}\n",
    ")\n",
    "\n",
    "html = f\"<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>{missing_info}</pre>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial analyses revealed that the buoys report at different, irregular frequencies. This makes simple partitioning and interpolation problematic, as it can lead to data leakage and an inconsistent time index. To solve this, a better data integration strategy is employed. The goal is to create a single, unified dataframe for the predictive model with a perfectly regular time index. The strategy is as follows:\n",
    "\n",
    "- **Isolate Stations:** Each station's data is isolated into its own dataframe. Duplicates are handled by grouping by the timestamp.\n",
    "\n",
    "- **Create a Master Time Index:** A new, perfectly regular 20-minute time index is created. This will serve as the backbone of the final dataframe, ensuring a consistent frequency for our model.\n",
    "\n",
    "- **Feature Engineering with `merge_asof`:** The data from each buoy is treated as a unique source of features. `pd.merge_asof` is used to get the most recent valid measurement from each buoy for every 20-minute timestamp in the master index. This avoids data leakage and intelligently combines the sparse data.\n",
    "\n",
    "- **Final Interpolation:** After assembly, `interpolate(method='time')` is used one last time to fill any small, remaining gaps in the final, unified time series.\n",
    "\n",
    "This approach creates a single, high-quality, feature-rich dataset that uses all available information without corrupting the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>The analysis of time gaps in <code>df_complete</code> shows the following distribution (in minutes):</p>\n",
       "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>date_time\n",
       "60.0     714\n",
       "120.0      8</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Isolate and Clean Each Station's Data Individually ---\n",
    "\n",
    "# Isolate data for each station\n",
    "df1 = df[df[\"station_id\"] == 2868187].copy()  # The \"master\" clock\n",
    "df2 = df[df[\"station_id\"] == 2868934].copy()  # The fast, 10-min wind station\n",
    "df3 = df[df[\"station_id\"] == 2888997].copy()  # The wave rider\n",
    "\n",
    "# Handle potential duplicates by grouping by timestamp and taking the mean.\n",
    "df1 = df1.groupby(\"date_time\").mean().sort_index()\n",
    "df2 = df2.groupby(\"date_time\").mean().sort_index()\n",
    "df3 = df3.groupby(\"date_time\").mean().sort_index()\n",
    "\n",
    "\n",
    "# --- 2. Build the Model DataFrame from the Master Clock ---\n",
    "\n",
    "df_complete = df1.copy()\n",
    "\n",
    "\n",
    "# --- 3. Engineer Features by Merging from Other Buoys ---\n",
    "\n",
    "# From Buoy 2 (the fast wind station) -> wind speed\n",
    "df2_features = df2[[\"wind_speed\"]].rename(columns={\"wind_speed\": \"buoy2_wind_speed\"})\n",
    "df_complete = pd.merge_asof(\n",
    "    left=df_complete,\n",
    "    right=df2_features,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    direction=\"backward\",\n",
    "    tolerance=pd.Timedelta(\n",
    "        \"35 minutes\"\n",
    "    ),  # Find the most recent reading within this window\n",
    ")\n",
    "\n",
    "# From Buoy 3 (the wave rider) -> wave height.\n",
    "df3_features = df3[[\"significant_wave_height_m\"]].rename(\n",
    "    columns={\"significant_wave_height_m\": \"buoy3_wave_height\"}\n",
    ")\n",
    "df_complete = pd.merge_asof(\n",
    "    left=df_complete,\n",
    "    right=df3_features,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    direction=\"backward\",\n",
    "    tolerance=pd.Timedelta(\"35 minutes\"),\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Handle Missing Values ---\n",
    "\n",
    "# Now, the only NaNs will be in the NEW columns (`buoy2_wind_speed`, `buoy3_wave_height`)\n",
    "# if there was no recent reading within the tolerance.\n",
    "df_complete[\"buoy2_wind_speed\"] = df_complete[\"buoy2_wind_speed\"].interpolate(\n",
    "    method=\"time\"\n",
    ")\n",
    "df_complete[\"buoy3_wave_height\"] = df_complete[\"buoy3_wave_height\"].interpolate(\n",
    "    method=\"time\"\n",
    ")\n",
    "\n",
    "# The primary dataframe's own columns can still be interpolated to handle its\n",
    "# own incidental missing values. Operating on the full dataframe is not ambiguous.\n",
    "df_complete.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "\n",
    "# --- 5. Final Cleanup ---\n",
    "\n",
    "# Drop any rows where a value could not be interpolated (usually at the start).\n",
    "df_complete.dropna(inplace=True)\n",
    "\n",
    "# Drop the redundant station_id column\n",
    "if \"station_id\" in df_complete.columns:\n",
    "    df_complete.drop(columns=\"station_id\", inplace=True)\n",
    "\n",
    "\n",
    "# --- 6. Display Final Sampling Rate ---\n",
    "\n",
    "time_deltas = df_complete.index.to_series().diff().dt.total_seconds() / 60\n",
    "\n",
    "# Display the frequency of each gap in minutes\n",
    "gap_counts = time_deltas.value_counts().sort_index()\n",
    "\n",
    "html = f\"\"\"\n",
    "<p>The analysis of time gaps in <code>df_complete</code> shows the following distribution (in minutes):</p>\n",
    "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>{gap_counts.to_string()}</pre>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p>Final Integrated Model DataFrame Info:</p>\n",
       "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'><class 'pandas.core.frame.DataFrame'>\n",
       "DatetimeIndex: 723 entries, 2013-04-18 00:50:00+00:00 to 2013-05-18 10:50:00+00:00\n",
       "Data columns (total 11 columns):\n",
       " #   Column                           Non-Null Count  Dtype  \n",
       "---  ------                           --------------  -----  \n",
       " 0   latitude_degrees_north           723 non-null    float64\n",
       " 1   longitude_degrees_east           723 non-null    float64\n",
       " 2   air_temperature_celsius          723 non-null    float64\n",
       " 3   sea_level_pressure_hpa           723 non-null    float64\n",
       " 4   gust_speed_mps                   723 non-null    float64\n",
       " 5   significant_wave_height_m        723 non-null    float64\n",
       " 6   dominant_wave_period_s           723 non-null    float64\n",
       " 7   sea_surface_temperature_celsius  723 non-null    float64\n",
       " 8   wind_speed                       723 non-null    float64\n",
       " 9   buoy2_wind_speed                 723 non-null    float64\n",
       " 10  buoy3_wave_height                723 non-null    float64\n",
       "dtypes: float64(11)\n",
       "memory usage: 67.8 KB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "buffer = io.StringIO()\n",
    "df_complete.info(buf=buffer)\n",
    "info_str = buffer.getvalue()\n",
    "\n",
    "html = f\"\"\"\n",
    "<p>Final Integrated Model DataFrame Info:</p>\n",
    "<pre style='font-family: monospace; border-style: solid; border-width: thin; padding:10px;'>{info_str}</pre>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.to_csv(\"../data/clean/NOAA_complete.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noaa-dataset-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
